{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "Sven Malvik&#39;s Blog",
  "language": "en",
  "home_page_url": "http://localhost:8080/",
  "feed_url": "http://localhost:8080/feed.json",
  "description": "Insights about AI leadership, tech, and trance music - sharing my journey at the intersection of technology and music.",
  "authors": [
    {
      "name": "Sven Malvik"
    }
  ],
  "items": [{
      "id": "http://localhost:8080/newsletter/ai-weekly-gpt-5-preparations-claude-code-limits-and-context-rot/",
      "url": "http://localhost:8080/newsletter/ai-weekly-gpt-5-preparations-claude-code-limits-and-context-rot/",
      "title": "AI Weekly: GPT-5 Preparations, Claude Code Limits, and Context Rot",
      "content_html": "<p>Welcome to this weekâ€™s AI newsletter! Weâ€™re seeing significant movement across the AI landscape with major announcements from OpenAI, Anthropic, and Mistral AI, plus some fascinating new tools and concepts emerging.</p><h2 id=\"ğŸš€-gpt-5-goes-live-what-matters-for-power-users\"><a href=\"#ğŸš€-gpt-5-goes-live-what-matters-for-power-users\" class=\"heading-anchor\">ğŸš€ GPT-5 Goes Live: What Matters for Power Users</a></h2><p>GPT-5 kicks off a new chapter by <em>blending fast and slow thinking</em>, mastering multi-modal content, and becoming a genuine â€œAI agentâ€ for complex productivity. If deep integrations and automation are your focusâ€”or you want the latest in smart AIâ€”this is a big leap. But for code-centric power users, keeping Claude 4 in your toolset still makes sense.</p><p><strong>Launch Date:</strong><br>OpenAI is officially launching GPT-5 in early August 2025, bringing several upgrades aimed at both developers and everyday professionals.</p><h3 id=\"key-features-at-a-glance\"><a href=\"#key-features-at-a-glance\" class=\"heading-anchor\"><strong>Key Features at a Glance</strong></a></h3><ul class=\"list\"><li><p><strong>Unified â€œSmart Modeâ€ Architecture:</strong><br>GPT-5 doesnâ€™t make users choose between fast but shallow (like older GPT-4o) or deep and reasoned (as in GPT-4.1) responses. Instead, it <em>automatically</em> blends quick thinking and high-level reasoning in a single session, adapting on the fly to what your prompt needsâ€”no manual toggling (does <em>not</em> revert to the â€œ03â€ logic, but is a new architecture that merges fast and slow approaches seamlessly).</p></li><li><p><strong>Expanded Context Window:</strong><br>Handles up to 1 million tokens, letting you upload books, codebases, or project archives for discussionâ€”yet beware â€œcontext rot:â€ if your docs arenâ€™t curated, performance can still degrade, so targeted snippets beat giant dumps.</p></li><li><p><strong>Multimodal Mastery:</strong><br>Natively understands and reasons about text, images, and audio in a unified conversation. For instance: Snap a whiteboard plus upload meeting notes and get actionable next steps.</p></li><li><p><strong>Agentic Capabilities &amp; Integrations:</strong><br>GPT-5 acts as a smart assistant: It can <strong>call APIs, run functions, and automate workflows</strong> inside your favorite software (calendar, email, even databases). This is genuine â€œagent modeâ€â€”not just answering queries, but <em>executing tasks</em> for you.</p></li></ul><h3 id=\"coding-benchmarks-gpt-5-vs-claude-4\"><a href=\"#coding-benchmarks-gpt-5-vs-claude-4\" class=\"heading-anchor\"><strong>Coding Benchmarks: GPT-5 vs Claude 4</strong></a></h3><ul class=\"list\"><li><p><strong>Coding Performance:</strong><br>On the SWE-bench benchmark, GPT-5 (in preview) hits ~39%, while <strong>Claude 4</strong> leads with ~53%â€”Claude still has the edge for hardcore coding and bug-fixing right now, especially on live repo tasks.</p></li><li><p><strong>Reasoning &amp; Multitask Learning:</strong><br>GPT-5â€™s average score on MMLU is a strong 89.6%â€”a bump up from GPT-4.1â€™s 86.4%â€”and excels at multi-step logic, making it trustworthy for legal and technical research.</p></li></ul><h3 id=\"when-should-you-use-gpt-5\"><a href=\"#when-should-you-use-gpt-5\" class=\"heading-anchor\"><strong>When Should You Use GPT-5?</strong></a></h3><ul class=\"list\"><li><p><strong>YES â€”</strong></p><ul class=\"list\"><li>If you need an AI to <em>take actions not just answer</em>: automate repetitive work, schedule meetings, summarize &amp; file docs.</li><li>When you require high accuracy, multi-modal support, or plan to integrate AI into other tools (via function calling/API).</li><li>For research, data analysis, and professional tasks where context, reliability, and smart automation matter.</li></ul></li><li><p><strong>SKIP (for now) â€”</strong></p><ul class=\"list\"><li>If youâ€™re a casual user or mainly want quick writing, basic Q&amp;A, or simple codingâ€”GPT-5â€™s upgrades may outpace your needs and cost more.</li><li>Hardcore coders: Claude 4 is currently stronger on code and repo-level troubleshooting, so consider both if coding is mission critical.</li></ul></li></ul><p>This represents the next major leap in large language model capabilities, potentially setting new benchmarks for AI performance across various tasks.</p><hr><h2 id=\"ğŸ“Š-anthropic-claude-code-rate-limits-whats-coming-and-why-it-counts\"><a href=\"#ğŸ“Š-anthropic-claude-code-rate-limits-whats-coming-and-why-it-counts\" class=\"heading-anchor\">ğŸ“Š Anthropic Claude Code Rate Limits: Whatâ€™s Coming and Why It Counts</a></h2><p><strong>From August 28, 2025, Anthropic is adding new weekly usage caps to Claude Code across all paid plans.</strong> In addition to 5-hour session resets, users now have hard weekly limits (e.g., 40â€“80 hours/week for Pro). A small handful of heavy users will need to either pace themselves or buy extra capacity.</p><p><strong>Why the change?</strong> Some were running non-stop workloads that strained Anthropicâ€™s servers and budget. This move is meant to curb abuse, protect average usersâ€™ experience, and keep prices sustainable.</p><p><strong>Compared to Cursor:</strong><br>Cursor recently switched to an API-credits model where costs pile up unpredictably and context limits interrupt big tasks. Users report surprise overage bills and smaller working windows. By contrast, Claude Codeâ€™s new limits are clearâ€”you know exactly what youâ€™re paying for with no hidden bills.</p><p><strong>User sentiment:</strong></p><ul class=\"list\"><li>Many welcome predictable caps and improved stability (â€œAt least I know my max billâ€).</li><li>Power users are frustrated by hard stops mid-project and vague usage tracking (â€œWish I could see my limit in real timeâ€).</li><li>Some are looking at alternatives, but most agree: better to have clear limits than nasty overages.</li></ul><p><strong>Bottom line:</strong><br>Claude Codeâ€™s weekly rate limits are a shift toward transparent, predictable cost controlâ€”a big plus over Cursorâ€™s recent changes, even if a few power users have to adapt.</p><hr><h2 id=\"ğŸ¤-what-the-mistral-ai-and-ntt-data-partnership-means-for-scandinavian-fintech\"><a href=\"#ğŸ¤-what-the-mistral-ai-and-ntt-data-partnership-means-for-scandinavian-fintech\" class=\"heading-anchor\">ğŸ¤ What the Mistral AI &amp; NTT Data Partnership Means for Scandinavian Fintech</a></h2><p>The recent alliance between Mistral AI and NTT Data could reshape the AI landscape for Scandinavian fintechs, offering new advantages in compliance, localization, and AI-powered innovation.</p><h3 id=\"1-data-sovereignty-and-nordic-compliance\"><a href=\"#1-data-sovereignty-and-nordic-compliance\" class=\"heading-anchor\">1. Data Sovereignty &amp; Nordic Compliance</a></h3><p>Fintechs in Scandinavia face rigorous privacy rules. This partnership promises AI solutions where all customer and transactional data stays within national/EU borders, ensuring full alignment with both EU and Nordic data laws.</p><h3 id=\"2-region-specific-customizable-ai\"><a href=\"#2-region-specific-customizable-ai\" class=\"heading-anchor\">2. Region-Specific, Customizable AI</a></h3><p>By focusing on open, high-performance models tailored for European markets, Scandinavian fintechs can deploy AI tools optimized for local languages and processesâ€”critical for delivering compliant, regionally relevant services.</p><h3 id=\"3-secure-enterprise-grade-cloud-options\"><a href=\"#3-secure-enterprise-grade-cloud-options\" class=\"heading-anchor\">3. Secure, Enterprise-Grade Cloud Options</a></h3><p>Private and sovereign cloud deployment options bypass reliance on US-based clouds, a common regulatory hurdle in the Nordics. End-to-end services mean even smaller fintechs can quickly implement AI without massive in-house teams.</p><h3 id=\"4-fast-compliant-customer-automation\"><a href=\"#4-fast-compliant-customer-automation\" class=\"heading-anchor\">4. Fast, Compliant Customer Automation</a></h3><p>Integration with NTT Dataâ€™s AI ecosystem lets fintechs automate onboarding, anti-fraud, and reporting processesâ€”speeding up compliance-driven innovations and audits while keeping transparency high.</p><h4 id=\"key-benefits-and-strategic-points\"><a href=\"#key-benefits-and-strategic-points\" class=\"heading-anchor\">Key Benefits &amp; Strategic Points</a></h4><ul class=\"list\"><li><strong>Rapid AI Adoption</strong>: Scandinavian fintechs can adopt advanced but compliant AI without common deployment delays.</li><li><strong>Sustainability Mindset</strong>: Efficient, sustainable AI models resonate with Nordic tech values.</li><li><strong>European Digital Path</strong>: Provides a high-quality, non-US alternative for critical AI projects.</li></ul><h4 id=\"things-to-watch\"><a href=\"#things-to-watch\" class=\"heading-anchor\">Things to Watch</a></h4><ul class=\"list\"><li><strong>Integration Hurdles</strong>: Tying new AI into legacy systems could be complex.</li><li><strong>Language Accuracy</strong>: Success hinges on strong support for Nordic languages.</li><li><strong>Market Focus</strong>: The Nordicsâ€™ role in early adoption will shape lasting impact.</li></ul><p>The partnership offers plug-and-play, compliance-ready, Scandinavian languageâ€“compatible AI platformsâ€”giving Nordic fintechs the tools to lead in secure, customer-focused financial services innovation.</p><hr><h2 id=\"ğŸ“š-chatgpt-study-mode-fintech-pros-shortcut-to-upskilling\"><a href=\"#ğŸ“š-chatgpt-study-mode-fintech-pros-shortcut-to-upskilling\" class=\"heading-anchor\">ğŸ“š ChatGPT Study Mode: Fintech Prosâ€™ Shortcut to Upskilling</a></h2><p><strong>ChatGPT Study Mode</strong> isnâ€™t just for studentsâ€”itâ€™s a quick, personalized learning tool that professionals in fintech can use to stay updated on new tech, regulations, and industry changes.</p><h3 id=\"why-use-study-mode-for-fintech\"><a href=\"#why-use-study-mode-for-fintech\" class=\"heading-anchor\">Why Use Study Mode for Fintech?</a></h3><ul class=\"list\"><li><strong>Digest Complex Changes</strong>: Breaks down compliance updates, tech specs, or case studies step by step, so you learn and rememberâ€”not just skim.</li><li><strong>Custom Fits to Projects</strong>: Add your own policy docs or code, get auto-generated quizzes or flashcards tailored to your daily work.</li><li><strong>24/7 Learning</strong>: Study at your own pace, any time, from anywhere.</li></ul><h3 id=\"proceed-with-awareness\"><a href=\"#proceed-with-awareness\" class=\"heading-anchor\">Proceed With Awareness</a></h3><ul class=\"list\"><li><strong>Double-Check Facts</strong>: Output isnâ€™t always perfect. Cross-reference AI insights with trusted fintech sources.</li><li><strong>Donâ€™t Skip Human Input</strong>: For strategy or high-stakes work, keep collaborating with colleagues.</li></ul><blockquote><p><strong>Bottom Line:</strong> ChatGPT Study Mode is a fast, customizable way for fintech teams to keep their edge. Use it to break down dense topics and build skillsâ€”just make sure to keep critical thinking in the workflow.</p></blockquote><h3 id=\"see-it-in-action\"><a href=\"#see-it-in-action\" class=\"heading-anchor\">See It in Action</a></h3><ul class=\"list\"><li><a href=\"https://www.youtube.com/watch?v=XDYilxy1dn8\" rel=\"noopener\">YouTube: ChatGPT Study Mode Demo</a><br><em>(A quick walkthrough of ChatGPTâ€™s guided learning mode showcasing how it supports knowledge checks and interactive prompts.)</em></li></ul><hr><h2 id=\"ğŸ”„-understanding-context-rot\"><a href=\"#ğŸ”„-understanding-context-rot\" class=\"heading-anchor\">ğŸ”„ Understanding â€œContext Rotâ€</a></h2><p><strong>What is â€œContext Rotâ€ and Why Does it Matter?</strong></p><p><em>Context rot</em> is a term used to describe how AI chatbots and virtual assistants gradually lose track of what matters in a conversation as more information piles up. Picture a chatbot that starts out sharp but, after many back-and-forth replies, gets confusedâ€”forgetting important points, mixing things up, or referencing outdated info.</p><p><strong>Why does this matter?</strong></p><ul class=\"list\"><li>As conversations or documents get longer, <em>AI can start making more mistakes</em> or offer less relevant answers.</li><li>This matters for everyone who uses AI for research, customer service, or any long-term chat: <strong>the quality of responses can quietly slide downhill</strong>.</li><li>If you know about context rot, you can spot when your AI assistant is â€œlosing the plotâ€â€”and then you can reset, clarify, or trim down what you share for sharper answers.</li></ul><p><strong>In short:</strong> <em>Context rot</em> is a hidden reason why your favorite AI assistant sometimes goes off track. Knowing it exists helps you manage expectations and get better results. If you use (or build) AI tools, you should careâ€”because more information isnâ€™t always better if the AI canâ€™t keep up.</p><hr><h2 id=\"ğŸ¤–-automated-browser-control-in-cursor-mcp\"><a href=\"#ğŸ¤–-automated-browser-control-in-cursor-mcp\" class=\"heading-anchor\">ğŸ¤– Automated Browser Control in Cursor MCP</a></h2><p>Cursorâ€™s MCP servers now bring <strong>true browser automation</strong> straight into your workflowâ€”no scripts needed. Just use natural language to tell Cursor to navigate, fill forms, grab screenshots, or scrape dataâ€”all automated in your real browser[1][2][3].</p><h3 id=\"why-it-matters\"><a href=\"#why-it-matters\" class=\"heading-anchor\">Why It Matters</a></h3><ul class=\"list\"><li><strong>No more manual repetition:</strong> Automate those browser tasks you hate, from test runs to routine data collection.</li><li><strong>Instant feedback:</strong> Trigger end-to-end tests, debug frontends, or collect logs right from your IDEâ€”no context switching.</li><li><strong>More power, less friction:</strong> Focus on shipping code, not copy-pasting data or clicking buttons repeatedly.</li></ul><h3 id=\"how-to-get-automation\"><a href=\"#how-to-get-automation\" class=\"heading-anchor\">How to Get Automation</a></h3><ol class=\"list\"><li><strong>Install the Browser Extension:</strong><br>Download the <em>Browser MCP</em> (or <em>BrowserTools MCP</em>) extension for Chrome (from GitHub or store)[1][2].</li><li><strong>Add an Automation MCP Server in Cursor:</strong><ul class=\"list\"><li>Go to Cursor Settings â†’ Features â†’ MCP Servers.</li><li>Click â€œAdd new MCP serverâ€ and enter a command, like:<pre class=\"language-plaintext\"><code class=\"language-plaintext\">npx @agentdeskai/browser-tools-mcp@latest</code></pre>or for Playwright:<pre class=\"language-plaintext\"><code class=\"language-plaintext\">npx @playwright/mcp@latest</code></pre></li><li>Save, refresh, and look for a green status light[2][3].</li></ul></li><li><strong>Start Automating:</strong><br>In Cursorâ€™s chat, type commands like:<blockquote><p>â€œOpen <a href=\"https://example.com\" rel=\"noopener\">https://example.com</a> and fill out the form.â€<br>Cursor handles the rest, orchestrating browser actions as requested[2].</p></blockquote></li></ol><h3 id=\"do-you-need-to-configure-anything-special\"><a href=\"#do-you-need-to-configure-anything-special\" class=\"heading-anchor\">Do You Need to Configure Anything Special?</a></h3><ul class=\"list\"><li><strong>Mostly familiar:</strong> If youâ€™ve set up MCP servers before, this is nearly the sameâ€”just add/enable the browser automation server and extension[2][3].</li><li><strong>Extension permissions:</strong> Accept any prompted permissions for automation.</li><li><strong>Use your real browser:</strong> Automation works in your own session, so it keeps you logged in and avoids CAPTCHAs[1].</li></ul><p><strong>Bottom line:</strong> If youâ€™re already using MCPs in Cursor, automation is a quick upgradeâ€”with serious productivity gains, especially for repetitive browser tasks and testing. Give it a try!</p><hr><h2 id=\"ğŸ§ -obsidian-and-notebooklm-integrationâ€”a-scandinavian-fintech-lens\"><a href=\"#ğŸ§ -obsidian-and-notebooklm-integrationâ€”a-scandinavian-fintech-lens\" class=\"heading-anchor\">ğŸ§  Obsidian &amp; NotebookLM Integrationâ€”A Scandinavian Fintech Lens</a></h2><p><strong>Core Integration Approach:</strong><br>This is not a seamless sync, but a manual workflow: export your Obsidian notes (typically as PDFs) and upload them to Google NotebookLM for AI analysis. Your data moves from private storage to Googleâ€™s cloud.</p><p><strong>Whatâ€™s Really New for Obsidian Users:</strong><br>Youâ€™ll need to leave Obsidianâ€™s local, private system and work partly in Googleâ€™s environment to access new features. This can change howâ€”and whereâ€”you manage your notes.</p><p><strong>Impact on Regulated Scandinavian Fintech:</strong><br>Transferring notes to Googleâ€™s servers may violate local and EU regulations (GDPR, PSD2, etc.). Sensitive client or business data should remain strictly local.</p><p><strong>Usability Limitations:</strong></p><ul class=\"list\"><li>No direct syncing; every update is a manual export/import.</li><li>You lose Obsidianâ€™s linking and plugin features within NotebookLM.</li><li>Notebooks in NotebookLM canâ€™t be cross-referenced.</li></ul><p><strong>Best Use Case:</strong><br>Only use this for <strong>non-sensitive</strong> contentâ€”like public research and general market information. Never process client or confidential data through NotebookLM: keep all regulated information in Obsidian.</p><hr><p><em>Thatâ€™s a wrap for this weekâ€™s AI developments! The pace of innovation continues to accelerate, with new tools and capabilities emerging regularly. Stay tuned for next weekâ€™s update.</em></p>",
      "date_published": "2025-08-01T00:00:00Z"
    }
    ,{
      "id": "http://localhost:8080/newsletter/how-i-write-git-commit-messages/",
      "url": "http://localhost:8080/newsletter/how-i-write-git-commit-messages/",
      "title": "How I write Git commit messages",
      "content_html": "<h1 id=\"crafting-better-git-commit-messages-with-ai-ğŸš€\"><a href=\"#crafting-better-git-commit-messages-with-ai-ğŸš€\" class=\"heading-anchor\">Crafting Better Git Commit Messages with AI ğŸš€</a></h1><p>Hey fellow developers! Today, I want to share something thatâ€™s been a game-changer in my daily workflow - how I craft meaningful Git commit messages using AI, without relying on GitHub Copilot.</p><h2 id=\"quick-method-in-cursor\"><a href=\"#quick-method-in-cursor\" class=\"heading-anchor\">Quick Method in Cursor</a></h2><p>Listen! ğŸ¯ While writing this post, I stumbled upon this quick way to generate commit messages:</p><ol class=\"list\"><li>Open Cursorâ€™s Source Control panel (look for the branch/Y-shaped icon in the sidebar)</li><li>Find the commit message input field</li><li>Click the AI-generated commit message button (it looks like a star â­)</li></ol><h2 id=\"my-personal-method-using-cursor-chat\"><a href=\"#my-personal-method-using-cursor-chat\" class=\"heading-anchor\">My Personal Method: Using Cursor Chat</a></h2><p>But hereâ€™s how I personally prefer to do it:</p><ol class=\"list\"><li>ğŸ“ Open Cursorâ€™s Chat</li><li>ğŸ” Ensure no file is in the context</li><li>âš¡ï¸ Type <code>@PR</code> and hit Enter (this adds @PR (Diff with Main Branch) to the context)</li><li>ğŸ’¬ Use this magic prompt:<pre class=\"language-plaintext\"><code class=\"language-plaintext\">You are an engineer who wants to commit changes to the remote codebase. Your job is it to provide a short commit message about the recent changes as a one-liner.</code></pre></li><li>âœ¨ Hit Enter and voilÃ  - perfect commit messages every time!</li></ol><p>Now that I think about it, I could probably create a Cursor extension to automate this whole process with a single keystroke. Maybe thatâ€™ll be my next project!</p><p>And thatâ€™s it! While both methods use AI to generate commit messages, thereâ€™s an interesting difference: my personal method always creates a concise one-liner, while the quick method might generate multi-line messages.</p><p>Listen! ğŸ’¡ I personally prefer the one-liner approach as it keeps my git history clean and scannable. But now you know both methods - try them out and see which style works better for your workflow!</p><p>~Sven</p>",
      "date_published": "2025-01-05T00:00:00Z"
    }
    ,{
      "id": "http://localhost:8080/newsletter/azure-api-management-challenges-finding-the-right-path/",
      "url": "http://localhost:8080/newsletter/azure-api-management-challenges-finding-the-right-path/",
      "title": "Azure API Management Challenges: Finding the Right Path",
      "content_html": "<p>Hey cloud architects! ğŸ‘‹</p><p>Following up on <a href=\"/blog/vippsi-gets-smarter-expanding-file-support/\">yesterdayâ€™s post</a> about our document processing system, I wanted to share some interesting challenges weâ€™ve encountered with Azure API Management (APIM) and the solutions weâ€™re exploring.</p><h2 id=\"the-challenge\"><a href=\"#the-challenge\" class=\"heading-anchor\">The Challenge</a></h2><p>While implementing our document processing system, weâ€™ve hit a limitation with Azure API Management: it struggles with payloads larger than 100kB. This presents an interesting architectural decision point for our system.</p><h2 id=\"attempted-solutions\"><a href=\"#attempted-solutions\" class=\"heading-anchor\">Attempted Solutions</a></h2><p>Thereâ€™s actually a policy-based approach in APIM that supposedly handles larger payloads:</p><pre class=\"language-xml\"><code class=\"language-xml\"><span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>validate-content</span> <span class=\"token attr-name\">unspecified-content-type-action</span><span class=\"token attr-value\"><span class=\"token punctuation attr-equals\">=</span><span class=\"token punctuation\">\"</span>ignore<span class=\"token punctuation\">\"</span></span> <span class=\"token attr-name\">max-size</span><span class=\"token attr-value\"><span class=\"token punctuation attr-equals\">=</span><span class=\"token punctuation\">\"</span>4194304<span class=\"token punctuation\">\"</span></span> <span class=\"token punctuation\">/&gt;</span></span></code></pre><p>However, this solution comes with its own set of challenges:</p><ul class=\"list\"><li>The policy doesnâ€™t seem to work reliably in practice</li><li>It adds another layer of complexity to our APIM policies</li><li>When combined with CORS handling, the policies become increasingly complex</li><li>Policy maintenance becomes a significant overhead</li></ul><h2 id=\"current-workaround\"><a href=\"#current-workaround\" class=\"heading-anchor\">Current Workaround</a></h2><p>As a temporary measure, weâ€™ve implemented a simple file size limit of 100kB. While this gets us moving, itâ€™s clearly not the long-term solution we need for handling larger documents effectively.</p><h2 id=\"the-path-forward\"><a href=\"#the-path-forward\" class=\"heading-anchor\">The Path Forward</a></h2><p>Iâ€™m exploring a more robust approach:</p><ul class=\"list\"><li>Moving files to Azure Storage Account</li><li>Maintaining only references in local IndexDB</li><li>Preserving user privacy by avoiding direct user-file associations</li><li>Bypassing APIM size limitations entirely</li></ul><p>While we could technically increase APIMâ€™s limit to 4MB through a support ticket, Iâ€™m leaning towards the Storage Account solution as it provides better scalability and architectural cleanliness. This would also help us avoid the complexity trap of managing intricate APIM policies.</p><h2 id=\"deep-dive-into-apim\"><a href=\"#deep-dive-into-apim\" class=\"heading-anchor\">Deep Dive into APIM</a></h2><p><picture><source type=\"image/webp\" srcset=\"/assets/images/PD6uaSZcxS-440.webp 440w\" sizes=\"90vw\"><img loading=\"lazy\" decoding=\"async\" alt=\"Mastering Azure API Management book cover\" src=\"/assets/images/PD6uaSZcxS-440.jpeg\" width=\"440\" height=\"621\"></picture></p><p>If youâ€™re interested in mastering Azure API Management and want to learn more about handling these kinds of challenges (and many others), check out my book <a href=\"https://www.amazon.com/Mastering-Azure-API-Management-Implementing/dp/1484280105\" rel=\"noopener\">â€œMastering Azure API Managementâ€</a>. It covers everything from basic setup to advanced scenarios, including detailed discussions about policies, security, and architectural best practices.</p><p>Stay tuned as we continue to evolve our document processing architecture!</p><p>~ Sven</p>",
      "date_published": "2025-01-03T00:00:00Z"
    }
    ,{
      "id": "http://localhost:8080/newsletter/vippsi-gets-smarter-expanding-file-support/",
      "url": "http://localhost:8080/newsletter/vippsi-gets-smarter-expanding-file-support/",
      "title": "Vippsi Gets Smarter: Expanding File Support",
      "content_html": "<p>Hey AI enthusiasts! ğŸ‘‹</p><p>Iâ€™ve been heads down working on Vippsi, our internal AI-powered conversation tool, and wanted to share what Iâ€™ve been up to. Imagine having a natural conversation with your entire organizationâ€™s knowledge base - asking questions about policies, documentation, or reports, and getting instant, contextual responses. Instead of scanning through hundreds of pages, you can simply ask questions in plain language and get precise answers drawn directly from your documents.</p><p>As a financial services company, Vipps MobilePay faces unique challenges with AI adoption. While solutions like OpenAI, Anthropic, or Google offer powerful capabilities, they arenâ€™t always suitable for our needs. Financial institutions require stringent compliance measures and data privacy guarantees, while keeping costs manageable at scale. Thatâ€™s why weâ€™re building Vippsi as a custom solution that puts privacy and compliance first.</p><h2 id=\"work-in-progress\"><a href=\"#work-in-progress\" class=\"heading-anchor\">Work in Progress</a></h2><p>Weâ€™re expanding Vippsi to support multiple file formats:</p><ul class=\"list\"><li>PDF documents</li><li>Word documents (DOCX)</li><li>PowerPoint presentations (PPTX)</li><li>Plain text files (TXT)</li><li>More formats coming soon</li></ul><h2 id=\"technical-architecture\"><a href=\"#technical-architecture\" class=\"heading-anchor\">Technical Architecture</a></h2><p>Let me explain how weâ€™re handling your documents behind the scenes. When you upload a file, itâ€™s processed in our backend but - and this is crucial - we never store anything. Think of it like having a conversation with someone who can read your document, answer your questions, but immediately forgets everything once youâ€™re done.</p><p>Your files and conversations stay right in your browser using IndexDB storage. When you chat about a document, we convert its content to base64 format (a way to represent binary data as text) and stream it back to your browser. This base64 version gets stored locally and is used in your future conversations, making subsequent interactions quick and efficient while keeping everything on your device.</p><p>Hereâ€™s where it gets clever: Each time you ask a new question, your browser sends the base64 encoded content, which is much smaller than the original document. No need for storage on our end, no conversation history to worry about - everything stays private and under your control.</p><h2 id=\"current-status-and-next-steps\"><a href=\"#current-status-and-next-steps\" class=\"heading-anchor\">Current Status &amp; Next Steps</a></h2><p>The backend architecture is ready, while frontend work continues:</p><ul class=\"list\"><li>Optimizing file upload experience</li><li>Implementing streaming UI feedback</li><li>Fine-tuning document conversations</li><li>Adding support for more formats</li><li>Enhancing conversation quality</li></ul><p>Stay tuned for more updates as we continue to enhance Vippsiâ€™s capabilities!</p><p>~ Sven</p>",
      "date_published": "2025-01-02T00:00:00Z"
    }
    ,{
      "id": "http://localhost:8080/newsletter/welcome-to-my-new-digital-home/",
      "url": "http://localhost:8080/newsletter/welcome-to-my-new-digital-home/",
      "title": "Welcome to My New Digital Home",
      "content_html": "<p>Hey everyone! ğŸ‘‹</p><p>Iâ€™m excited to launch my new website where Iâ€™ll be sharing my journey at the intersection of AI and trance music. This space will be my digital garden where I cultivate ideas, share experiences, and connect with like-minded people.</p><h2 id=\"what-to-expect\"><a href=\"#what-to-expect\" class=\"heading-anchor\">What to Expect</a></h2><p>Hereâ€™s what youâ€™ll find on this site:</p><ul class=\"list\"><li><strong>AI Leadership</strong>: Insights from my work leading the AI Platform Team at Vipps MobilePay, where weâ€™re using AI to simplify complex workflows</li><li><strong>Trance Music</strong>: Weekly updates about my latest mixes, featuring both classic anthems and fresh progressive tunes</li><li><strong>Tech Insights</strong>: Thoughts about technology, leadership, and the future of AI</li><li><strong>Personal Updates</strong>: Stories and experiences from my life in Norway</li></ul><h2 id=\"why-this-site\"><a href=\"#why-this-site\" class=\"heading-anchor\">Why This Site?</a></h2><p>I believe in the power of sharing knowledge and experiences. Whether itâ€™s discussing how weâ€™re implementing AI solutions at Vipps MobilePay or sharing the story behind my latest trance mix, this site is where Iâ€™ll keep track of my journey.</p><h2 id=\"stay-connected\"><a href=\"#stay-connected\" class=\"heading-anchor\">Stay Connected</a></h2><p>The best way to keep up with my content is to:</p><ul class=\"list\"><li>Follow me on <a href=\"https://www.linkedin.com/in/svenmalvik\" rel=\"noopener\">LinkedIn</a> for tech updates</li><li>Subscribe to my <a href=\"https://www.youtube.com/@svenmalvik\" rel=\"noopener\">YouTube channel</a> for weekly trance mixes</li><li>Check back here regularly for in-depth articles and insights</li></ul><p>Looking forward to sharing this journey with you!</p><p>~ Sven</p>",
      "date_published": "2025-01-01T00:00:00Z"
    }
    
  ]
}