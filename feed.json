{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "Sven Malvik&#39;s Blog",
  "language": "en",
  "home_page_url": "http://localhost:8080/",
  "feed_url": "http://localhost:8080/feed.json",
  "description": "Insights about AI leadership, tech, and trance music - sharing my journey at the intersection of technology and music.",
  "authors": [
    {
      "name": "Sven Malvik"
    }
  ],
  "items": [{
      "id": "http://localhost:8080/newsletter/ai-weekly-gpt-5-preparations-claude-code-limits-and-context-rot/",
      "url": "http://localhost:8080/newsletter/ai-weekly-gpt-5-preparations-claude-code-limits-and-context-rot/",
      "title": "AI Weekly: GPT-5 Preparations, Claude Code Limits, and Context Rot",
      "content_html": "<p>Welcome to this week‚Äôs AI newsletter! We‚Äôre seeing significant movement across the AI landscape with major announcements from OpenAI, Anthropic, and Mistral AI, plus some fascinating new tools and concepts emerging.</p><h2 id=\"üöÄ-gpt-5-goes-live-what-matters-for-power-users\"><a href=\"#üöÄ-gpt-5-goes-live-what-matters-for-power-users\" class=\"heading-anchor\">üöÄ GPT-5 Goes Live: What Matters for Power Users</a></h2><p>GPT-5 kicks off a new chapter by <em>blending fast and slow thinking</em>, mastering multi-modal content, and becoming a genuine ‚ÄúAI agent‚Äù for complex productivity. If deep integrations and automation are your focus‚Äîor you want the latest in smart AI‚Äîthis is a big leap. But for code-centric power users, keeping Claude 4 in your toolset still makes sense.</p><p><strong>Launch Date:</strong><br>OpenAI is officially launching GPT-5 in early August 2025, bringing several upgrades aimed at both developers and everyday professionals.</p><h3 id=\"key-features-at-a-glance\"><a href=\"#key-features-at-a-glance\" class=\"heading-anchor\"><strong>Key Features at a Glance</strong></a></h3><ul class=\"list\"><li><p><strong>Unified ‚ÄúSmart Mode‚Äù Architecture:</strong><br>GPT-5 doesn‚Äôt make users choose between fast but shallow (like older GPT-4o) or deep and reasoned (as in GPT-4.1) responses. Instead, it <em>automatically</em> blends quick thinking and high-level reasoning in a single session, adapting on the fly to what your prompt needs‚Äîno manual toggling (does <em>not</em> revert to the ‚Äú03‚Äù logic, but is a new architecture that merges fast and slow approaches seamlessly).</p></li><li><p><strong>Expanded Context Window:</strong><br>Handles up to 1 million tokens, letting you upload books, codebases, or project archives for discussion‚Äîyet beware ‚Äúcontext rot:‚Äù if your docs aren‚Äôt curated, performance can still degrade, so targeted snippets beat giant dumps.</p></li><li><p><strong>Multimodal Mastery:</strong><br>Natively understands and reasons about text, images, and audio in a unified conversation. For instance: Snap a whiteboard plus upload meeting notes and get actionable next steps.</p></li><li><p><strong>Agentic Capabilities &amp; Integrations:</strong><br>GPT-5 acts as a smart assistant: It can <strong>call APIs, run functions, and automate workflows</strong> inside your favorite software (calendar, email, even databases). This is genuine ‚Äúagent mode‚Äù‚Äînot just answering queries, but <em>executing tasks</em> for you.</p></li></ul><h3 id=\"coding-benchmarks-gpt-5-vs-claude-4\"><a href=\"#coding-benchmarks-gpt-5-vs-claude-4\" class=\"heading-anchor\"><strong>Coding Benchmarks: GPT-5 vs Claude 4</strong></a></h3><ul class=\"list\"><li><p><strong>Coding Performance:</strong><br>On the SWE-bench benchmark, GPT-5 (in preview) hits ~39%, while <strong>Claude 4</strong> leads with ~53%‚ÄîClaude still has the edge for hardcore coding and bug-fixing right now, especially on live repo tasks.</p></li><li><p><strong>Reasoning &amp; Multitask Learning:</strong><br>GPT-5‚Äôs average score on MMLU is a strong 89.6%‚Äîa bump up from GPT-4.1‚Äôs 86.4%‚Äîand excels at multi-step logic, making it trustworthy for legal and technical research.</p></li></ul><h3 id=\"when-should-you-use-gpt-5\"><a href=\"#when-should-you-use-gpt-5\" class=\"heading-anchor\"><strong>When Should You Use GPT-5?</strong></a></h3><ul class=\"list\"><li><p><strong>YES ‚Äî</strong></p><ul class=\"list\"><li>If you need an AI to <em>take actions not just answer</em>: automate repetitive work, schedule meetings, summarize &amp; file docs.</li><li>When you require high accuracy, multi-modal support, or plan to integrate AI into other tools (via function calling/API).</li><li>For research, data analysis, and professional tasks where context, reliability, and smart automation matter.</li></ul></li><li><p><strong>SKIP (for now) ‚Äî</strong></p><ul class=\"list\"><li>If you‚Äôre a casual user or mainly want quick writing, basic Q&amp;A, or simple coding‚ÄîGPT-5‚Äôs upgrades may outpace your needs and cost more.</li><li>Hardcore coders: Claude 4 is currently stronger on code and repo-level troubleshooting, so consider both if coding is mission critical.</li></ul></li></ul><p>This represents the next major leap in large language model capabilities, potentially setting new benchmarks for AI performance across various tasks.</p><hr><h2 id=\"üìä-anthropic-claude-code-rate-limits-whats-coming-and-why-it-counts\"><a href=\"#üìä-anthropic-claude-code-rate-limits-whats-coming-and-why-it-counts\" class=\"heading-anchor\">üìä Anthropic Claude Code Rate Limits: What‚Äôs Coming and Why It Counts</a></h2><p><strong>From August 28, 2025, Anthropic is adding new weekly usage caps to Claude Code across all paid plans.</strong> In addition to 5-hour session resets, users now have hard weekly limits (e.g., 40‚Äì80 hours/week for Pro). A small handful of heavy users will need to either pace themselves or buy extra capacity.</p><p><strong>Why the change?</strong> Some were running non-stop workloads that strained Anthropic‚Äôs servers and budget. This move is meant to curb abuse, protect average users‚Äô experience, and keep prices sustainable.</p><p><strong>Compared to Cursor:</strong><br>Cursor recently switched to an API-credits model where costs pile up unpredictably and context limits interrupt big tasks. Users report surprise overage bills and smaller working windows. By contrast, Claude Code‚Äôs new limits are clear‚Äîyou know exactly what you‚Äôre paying for with no hidden bills.</p><p><strong>User sentiment:</strong></p><ul class=\"list\"><li>Many welcome predictable caps and improved stability (‚ÄúAt least I know my max bill‚Äù).</li><li>Power users are frustrated by hard stops mid-project and vague usage tracking (‚ÄúWish I could see my limit in real time‚Äù).</li><li>Some are looking at alternatives, but most agree: better to have clear limits than nasty overages.</li></ul><p><strong>Bottom line:</strong><br>Claude Code‚Äôs weekly rate limits are a shift toward transparent, predictable cost control‚Äîa big plus over Cursor‚Äôs recent changes, even if a few power users have to adapt.</p><hr><h2 id=\"ü§ù-what-the-mistral-ai-and-ntt-data-partnership-means-for-scandinavian-fintech\"><a href=\"#ü§ù-what-the-mistral-ai-and-ntt-data-partnership-means-for-scandinavian-fintech\" class=\"heading-anchor\">ü§ù What the Mistral AI &amp; NTT Data Partnership Means for Scandinavian Fintech</a></h2><p>The recent alliance between Mistral AI and NTT Data could reshape the AI landscape for Scandinavian fintechs, offering new advantages in compliance, localization, and AI-powered innovation.</p><h3 id=\"1-data-sovereignty-and-nordic-compliance\"><a href=\"#1-data-sovereignty-and-nordic-compliance\" class=\"heading-anchor\">1. Data Sovereignty &amp; Nordic Compliance</a></h3><p>Fintechs in Scandinavia face rigorous privacy rules. This partnership promises AI solutions where all customer and transactional data stays within national/EU borders, ensuring full alignment with both EU and Nordic data laws.</p><h3 id=\"2-region-specific-customizable-ai\"><a href=\"#2-region-specific-customizable-ai\" class=\"heading-anchor\">2. Region-Specific, Customizable AI</a></h3><p>By focusing on open, high-performance models tailored for European markets, Scandinavian fintechs can deploy AI tools optimized for local languages and processes‚Äîcritical for delivering compliant, regionally relevant services.</p><h3 id=\"3-secure-enterprise-grade-cloud-options\"><a href=\"#3-secure-enterprise-grade-cloud-options\" class=\"heading-anchor\">3. Secure, Enterprise-Grade Cloud Options</a></h3><p>Private and sovereign cloud deployment options bypass reliance on US-based clouds, a common regulatory hurdle in the Nordics. End-to-end services mean even smaller fintechs can quickly implement AI without massive in-house teams.</p><h3 id=\"4-fast-compliant-customer-automation\"><a href=\"#4-fast-compliant-customer-automation\" class=\"heading-anchor\">4. Fast, Compliant Customer Automation</a></h3><p>Integration with NTT Data‚Äôs AI ecosystem lets fintechs automate onboarding, anti-fraud, and reporting processes‚Äîspeeding up compliance-driven innovations and audits while keeping transparency high.</p><h4 id=\"key-benefits-and-strategic-points\"><a href=\"#key-benefits-and-strategic-points\" class=\"heading-anchor\">Key Benefits &amp; Strategic Points</a></h4><ul class=\"list\"><li><strong>Rapid AI Adoption</strong>: Scandinavian fintechs can adopt advanced but compliant AI without common deployment delays.</li><li><strong>Sustainability Mindset</strong>: Efficient, sustainable AI models resonate with Nordic tech values.</li><li><strong>European Digital Path</strong>: Provides a high-quality, non-US alternative for critical AI projects.</li></ul><h4 id=\"things-to-watch\"><a href=\"#things-to-watch\" class=\"heading-anchor\">Things to Watch</a></h4><ul class=\"list\"><li><strong>Integration Hurdles</strong>: Tying new AI into legacy systems could be complex.</li><li><strong>Language Accuracy</strong>: Success hinges on strong support for Nordic languages.</li><li><strong>Market Focus</strong>: The Nordics‚Äô role in early adoption will shape lasting impact.</li></ul><p>The partnership offers plug-and-play, compliance-ready, Scandinavian language‚Äìcompatible AI platforms‚Äîgiving Nordic fintechs the tools to lead in secure, customer-focused financial services innovation.</p><hr><h2 id=\"üìö-chatgpt-study-mode-fintech-pros-shortcut-to-upskilling\"><a href=\"#üìö-chatgpt-study-mode-fintech-pros-shortcut-to-upskilling\" class=\"heading-anchor\">üìö ChatGPT Study Mode: Fintech Pros‚Äô Shortcut to Upskilling</a></h2><p><strong>ChatGPT Study Mode</strong> isn‚Äôt just for students‚Äîit‚Äôs a quick, personalized learning tool that professionals in fintech can use to stay updated on new tech, regulations, and industry changes.</p><h3 id=\"why-use-study-mode-for-fintech\"><a href=\"#why-use-study-mode-for-fintech\" class=\"heading-anchor\">Why Use Study Mode for Fintech?</a></h3><ul class=\"list\"><li><strong>Digest Complex Changes</strong>: Breaks down compliance updates, tech specs, or case studies step by step, so you learn and remember‚Äînot just skim.</li><li><strong>Custom Fits to Projects</strong>: Add your own policy docs or code, get auto-generated quizzes or flashcards tailored to your daily work.</li><li><strong>24/7 Learning</strong>: Study at your own pace, any time, from anywhere.</li></ul><h3 id=\"proceed-with-awareness\"><a href=\"#proceed-with-awareness\" class=\"heading-anchor\">Proceed With Awareness</a></h3><ul class=\"list\"><li><strong>Double-Check Facts</strong>: Output isn‚Äôt always perfect. Cross-reference AI insights with trusted fintech sources.</li><li><strong>Don‚Äôt Skip Human Input</strong>: For strategy or high-stakes work, keep collaborating with colleagues.</li></ul><blockquote><p><strong>Bottom Line:</strong> ChatGPT Study Mode is a fast, customizable way for fintech teams to keep their edge. Use it to break down dense topics and build skills‚Äîjust make sure to keep critical thinking in the workflow.</p></blockquote><h3 id=\"see-it-in-action\"><a href=\"#see-it-in-action\" class=\"heading-anchor\">See It in Action</a></h3><ul class=\"list\"><li><a href=\"https://www.youtube.com/watch?v=XDYilxy1dn8\" rel=\"noopener\">YouTube: ChatGPT Study Mode Demo</a><br><em>(A quick walkthrough of ChatGPT‚Äôs guided learning mode showcasing how it supports knowledge checks and interactive prompts.)</em></li></ul><hr><h2 id=\"üîÑ-understanding-context-rot\"><a href=\"#üîÑ-understanding-context-rot\" class=\"heading-anchor\">üîÑ Understanding ‚ÄúContext Rot‚Äù</a></h2><p><strong>What is ‚ÄúContext Rot‚Äù and Why Does it Matter?</strong></p><p><em>Context rot</em> is a term used to describe how AI chatbots and virtual assistants gradually lose track of what matters in a conversation as more information piles up. Picture a chatbot that starts out sharp but, after many back-and-forth replies, gets confused‚Äîforgetting important points, mixing things up, or referencing outdated info.</p><p><strong>Why does this matter?</strong></p><ul class=\"list\"><li>As conversations or documents get longer, <em>AI can start making more mistakes</em> or offer less relevant answers.</li><li>This matters for everyone who uses AI for research, customer service, or any long-term chat: <strong>the quality of responses can quietly slide downhill</strong>.</li><li>If you know about context rot, you can spot when your AI assistant is ‚Äúlosing the plot‚Äù‚Äîand then you can reset, clarify, or trim down what you share for sharper answers.</li></ul><p><strong>In short:</strong> <em>Context rot</em> is a hidden reason why your favorite AI assistant sometimes goes off track. Knowing it exists helps you manage expectations and get better results. If you use (or build) AI tools, you should care‚Äîbecause more information isn‚Äôt always better if the AI can‚Äôt keep up.</p><hr><h2 id=\"ü§ñ-automated-browser-control-in-cursor-mcp\"><a href=\"#ü§ñ-automated-browser-control-in-cursor-mcp\" class=\"heading-anchor\">ü§ñ Automated Browser Control in Cursor MCP</a></h2><p>Cursor‚Äôs MCP servers now bring <strong>true browser automation</strong> straight into your workflow‚Äîno scripts needed. Just use natural language to tell Cursor to navigate, fill forms, grab screenshots, or scrape data‚Äîall automated in your real browser[1][2][3].</p><h3 id=\"why-it-matters\"><a href=\"#why-it-matters\" class=\"heading-anchor\">Why It Matters</a></h3><ul class=\"list\"><li><strong>No more manual repetition:</strong> Automate those browser tasks you hate, from test runs to routine data collection.</li><li><strong>Instant feedback:</strong> Trigger end-to-end tests, debug frontends, or collect logs right from your IDE‚Äîno context switching.</li><li><strong>More power, less friction:</strong> Focus on shipping code, not copy-pasting data or clicking buttons repeatedly.</li></ul><h3 id=\"how-to-get-automation\"><a href=\"#how-to-get-automation\" class=\"heading-anchor\">How to Get Automation</a></h3><ol class=\"list\"><li><strong>Install the Browser Extension:</strong><br>Download the <em>Browser MCP</em> (or <em>BrowserTools MCP</em>) extension for Chrome (from GitHub or store)[1][2].</li><li><strong>Add an Automation MCP Server in Cursor:</strong><ul class=\"list\"><li>Go to Cursor Settings ‚Üí Features ‚Üí MCP Servers.</li><li>Click ‚ÄúAdd new MCP server‚Äù and enter a command, like:<pre class=\"language-plaintext\"><code class=\"language-plaintext\">npx @agentdeskai/browser-tools-mcp@latest</code></pre>or for Playwright:<pre class=\"language-plaintext\"><code class=\"language-plaintext\">npx @playwright/mcp@latest</code></pre></li><li>Save, refresh, and look for a green status light[2][3].</li></ul></li><li><strong>Start Automating:</strong><br>In Cursor‚Äôs chat, type commands like:<blockquote><p>‚ÄúOpen <a href=\"https://example.com\" rel=\"noopener\">https://example.com</a> and fill out the form.‚Äù<br>Cursor handles the rest, orchestrating browser actions as requested[2].</p></blockquote></li></ol><h3 id=\"do-you-need-to-configure-anything-special\"><a href=\"#do-you-need-to-configure-anything-special\" class=\"heading-anchor\">Do You Need to Configure Anything Special?</a></h3><ul class=\"list\"><li><strong>Mostly familiar:</strong> If you‚Äôve set up MCP servers before, this is nearly the same‚Äîjust add/enable the browser automation server and extension[2][3].</li><li><strong>Extension permissions:</strong> Accept any prompted permissions for automation.</li><li><strong>Use your real browser:</strong> Automation works in your own session, so it keeps you logged in and avoids CAPTCHAs[1].</li></ul><p><strong>Bottom line:</strong> If you‚Äôre already using MCPs in Cursor, automation is a quick upgrade‚Äîwith serious productivity gains, especially for repetitive browser tasks and testing. Give it a try!</p><hr><h2 id=\"üß†-obsidian-and-notebooklm-integration‚Äîa-scandinavian-fintech-lens\"><a href=\"#üß†-obsidian-and-notebooklm-integration‚Äîa-scandinavian-fintech-lens\" class=\"heading-anchor\">üß† Obsidian &amp; NotebookLM Integration‚ÄîA Scandinavian Fintech Lens</a></h2><p><strong>Core Integration Approach:</strong><br>This is not a seamless sync, but a manual workflow: export your Obsidian notes (typically as PDFs) and upload them to Google NotebookLM for AI analysis. Your data moves from private storage to Google‚Äôs cloud.</p><p><strong>What‚Äôs Really New for Obsidian Users:</strong><br>You‚Äôll need to leave Obsidian‚Äôs local, private system and work partly in Google‚Äôs environment to access new features. This can change how‚Äîand where‚Äîyou manage your notes.</p><p><strong>Impact on Regulated Scandinavian Fintech:</strong><br>Transferring notes to Google‚Äôs servers may violate local and EU regulations (GDPR, PSD2, etc.). Sensitive client or business data should remain strictly local.</p><p><strong>Usability Limitations:</strong></p><ul class=\"list\"><li>No direct syncing; every update is a manual export/import.</li><li>You lose Obsidian‚Äôs linking and plugin features within NotebookLM.</li><li>Notebooks in NotebookLM can‚Äôt be cross-referenced.</li></ul><p><strong>Best Use Case:</strong><br>Only use this for <strong>non-sensitive</strong> content‚Äîlike public research and general market information. Never process client or confidential data through NotebookLM: keep all regulated information in Obsidian.</p><hr><p><em>That‚Äôs a wrap for this week‚Äôs AI developments! The pace of innovation continues to accelerate, with new tools and capabilities emerging regularly. Stay tuned for next week‚Äôs update.</em></p>",
      "date_published": "2025-08-01T00:00:00Z"
    }
    ,{
      "id": "http://localhost:8080/newsletter/how-i-write-git-commit-messages/",
      "url": "http://localhost:8080/newsletter/how-i-write-git-commit-messages/",
      "title": "How I write Git commit messages",
      "content_html": "<h1 id=\"crafting-better-git-commit-messages-with-ai-üöÄ\"><a href=\"#crafting-better-git-commit-messages-with-ai-üöÄ\" class=\"heading-anchor\">Crafting Better Git Commit Messages with AI üöÄ</a></h1><p>Hey fellow developers! Today, I want to share something that‚Äôs been a game-changer in my daily workflow - how I craft meaningful Git commit messages using AI, without relying on GitHub Copilot.</p><h2 id=\"quick-method-in-cursor\"><a href=\"#quick-method-in-cursor\" class=\"heading-anchor\">Quick Method in Cursor</a></h2><p>Listen! üéØ While writing this post, I stumbled upon this quick way to generate commit messages:</p><ol class=\"list\"><li>Open Cursor‚Äôs Source Control panel (look for the branch/Y-shaped icon in the sidebar)</li><li>Find the commit message input field</li><li>Click the AI-generated commit message button (it looks like a star ‚≠ê)</li></ol><h2 id=\"my-personal-method-using-cursor-chat\"><a href=\"#my-personal-method-using-cursor-chat\" class=\"heading-anchor\">My Personal Method: Using Cursor Chat</a></h2><p>But here‚Äôs how I personally prefer to do it:</p><ol class=\"list\"><li>üìù Open Cursor‚Äôs Chat</li><li>üîç Ensure no file is in the context</li><li>‚ö°Ô∏è Type <code>@PR</code> and hit Enter (this adds @PR (Diff with Main Branch) to the context)</li><li>üí¨ Use this magic prompt:<pre class=\"language-plaintext\"><code class=\"language-plaintext\">You are an engineer who wants to commit changes to the remote codebase. Your job is it to provide a short commit message about the recent changes as a one-liner.</code></pre></li><li>‚ú® Hit Enter and voil√† - perfect commit messages every time!</li></ol><p>Now that I think about it, I could probably create a Cursor extension to automate this whole process with a single keystroke. Maybe that‚Äôll be my next project!</p><p>And that‚Äôs it! While both methods use AI to generate commit messages, there‚Äôs an interesting difference: my personal method always creates a concise one-liner, while the quick method might generate multi-line messages.</p><p>Listen! üí° I personally prefer the one-liner approach as it keeps my git history clean and scannable. But now you know both methods - try them out and see which style works better for your workflow!</p><p>~Sven</p>",
      "date_published": "2025-01-05T00:00:00Z"
    }
    ,{
      "id": "http://localhost:8080/newsletter/azure-api-management-challenges-finding-the-right-path/",
      "url": "http://localhost:8080/newsletter/azure-api-management-challenges-finding-the-right-path/",
      "title": "Azure API Management Challenges: Finding the Right Path",
      "content_html": "<p>Hey cloud architects! üëã</p><p>Following up on <a href=\"/blog/vippsi-gets-smarter-expanding-file-support/\">yesterday‚Äôs post</a> about our document processing system, I wanted to share some interesting challenges we‚Äôve encountered with Azure API Management (APIM) and the solutions we‚Äôre exploring.</p><h2 id=\"the-challenge\"><a href=\"#the-challenge\" class=\"heading-anchor\">The Challenge</a></h2><p>While implementing our document processing system, we‚Äôve hit a limitation with Azure API Management: it struggles with payloads larger than 100kB. This presents an interesting architectural decision point for our system.</p><h2 id=\"attempted-solutions\"><a href=\"#attempted-solutions\" class=\"heading-anchor\">Attempted Solutions</a></h2><p>There‚Äôs actually a policy-based approach in APIM that supposedly handles larger payloads:</p><pre class=\"language-xml\"><code class=\"language-xml\"><span class=\"token tag\"><span class=\"token tag\"><span class=\"token punctuation\">&lt;</span>validate-content</span> <span class=\"token attr-name\">unspecified-content-type-action</span><span class=\"token attr-value\"><span class=\"token punctuation attr-equals\">=</span><span class=\"token punctuation\">\"</span>ignore<span class=\"token punctuation\">\"</span></span> <span class=\"token attr-name\">max-size</span><span class=\"token attr-value\"><span class=\"token punctuation attr-equals\">=</span><span class=\"token punctuation\">\"</span>4194304<span class=\"token punctuation\">\"</span></span> <span class=\"token punctuation\">/&gt;</span></span></code></pre><p>However, this solution comes with its own set of challenges:</p><ul class=\"list\"><li>The policy doesn‚Äôt seem to work reliably in practice</li><li>It adds another layer of complexity to our APIM policies</li><li>When combined with CORS handling, the policies become increasingly complex</li><li>Policy maintenance becomes a significant overhead</li></ul><h2 id=\"current-workaround\"><a href=\"#current-workaround\" class=\"heading-anchor\">Current Workaround</a></h2><p>As a temporary measure, we‚Äôve implemented a simple file size limit of 100kB. While this gets us moving, it‚Äôs clearly not the long-term solution we need for handling larger documents effectively.</p><h2 id=\"the-path-forward\"><a href=\"#the-path-forward\" class=\"heading-anchor\">The Path Forward</a></h2><p>I‚Äôm exploring a more robust approach:</p><ul class=\"list\"><li>Moving files to Azure Storage Account</li><li>Maintaining only references in local IndexDB</li><li>Preserving user privacy by avoiding direct user-file associations</li><li>Bypassing APIM size limitations entirely</li></ul><p>While we could technically increase APIM‚Äôs limit to 4MB through a support ticket, I‚Äôm leaning towards the Storage Account solution as it provides better scalability and architectural cleanliness. This would also help us avoid the complexity trap of managing intricate APIM policies.</p><h2 id=\"deep-dive-into-apim\"><a href=\"#deep-dive-into-apim\" class=\"heading-anchor\">Deep Dive into APIM</a></h2><p><picture><source type=\"image/webp\" srcset=\"/assets/images/PD6uaSZcxS-440.webp 440w\" sizes=\"90vw\"><img loading=\"lazy\" decoding=\"async\" alt=\"Mastering Azure API Management book cover\" src=\"/assets/images/PD6uaSZcxS-440.jpeg\" width=\"440\" height=\"621\"></picture></p><p>If you‚Äôre interested in mastering Azure API Management and want to learn more about handling these kinds of challenges (and many others), check out my book <a href=\"https://www.amazon.com/Mastering-Azure-API-Management-Implementing/dp/1484280105\" rel=\"noopener\">‚ÄúMastering Azure API Management‚Äù</a>. It covers everything from basic setup to advanced scenarios, including detailed discussions about policies, security, and architectural best practices.</p><p>Stay tuned as we continue to evolve our document processing architecture!</p><p>~ Sven</p>",
      "date_published": "2025-01-03T00:00:00Z"
    }
    ,{
      "id": "http://localhost:8080/newsletter/vippsi-gets-smarter-expanding-file-support/",
      "url": "http://localhost:8080/newsletter/vippsi-gets-smarter-expanding-file-support/",
      "title": "Vippsi Gets Smarter: Expanding File Support",
      "content_html": "<p>Hey AI enthusiasts! üëã</p><p>I‚Äôve been heads down working on Vippsi, our internal AI-powered conversation tool, and wanted to share what I‚Äôve been up to. Imagine having a natural conversation with your entire organization‚Äôs knowledge base - asking questions about policies, documentation, or reports, and getting instant, contextual responses. Instead of scanning through hundreds of pages, you can simply ask questions in plain language and get precise answers drawn directly from your documents.</p><p>As a financial services company, Vipps MobilePay faces unique challenges with AI adoption. While solutions like OpenAI, Anthropic, or Google offer powerful capabilities, they aren‚Äôt always suitable for our needs. Financial institutions require stringent compliance measures and data privacy guarantees, while keeping costs manageable at scale. That‚Äôs why we‚Äôre building Vippsi as a custom solution that puts privacy and compliance first.</p><h2 id=\"work-in-progress\"><a href=\"#work-in-progress\" class=\"heading-anchor\">Work in Progress</a></h2><p>We‚Äôre expanding Vippsi to support multiple file formats:</p><ul class=\"list\"><li>PDF documents</li><li>Word documents (DOCX)</li><li>PowerPoint presentations (PPTX)</li><li>Plain text files (TXT)</li><li>More formats coming soon</li></ul><h2 id=\"technical-architecture\"><a href=\"#technical-architecture\" class=\"heading-anchor\">Technical Architecture</a></h2><p>Let me explain how we‚Äôre handling your documents behind the scenes. When you upload a file, it‚Äôs processed in our backend but - and this is crucial - we never store anything. Think of it like having a conversation with someone who can read your document, answer your questions, but immediately forgets everything once you‚Äôre done.</p><p>Your files and conversations stay right in your browser using IndexDB storage. When you chat about a document, we convert its content to base64 format (a way to represent binary data as text) and stream it back to your browser. This base64 version gets stored locally and is used in your future conversations, making subsequent interactions quick and efficient while keeping everything on your device.</p><p>Here‚Äôs where it gets clever: Each time you ask a new question, your browser sends the base64 encoded content, which is much smaller than the original document. No need for storage on our end, no conversation history to worry about - everything stays private and under your control.</p><h2 id=\"current-status-and-next-steps\"><a href=\"#current-status-and-next-steps\" class=\"heading-anchor\">Current Status &amp; Next Steps</a></h2><p>The backend architecture is ready, while frontend work continues:</p><ul class=\"list\"><li>Optimizing file upload experience</li><li>Implementing streaming UI feedback</li><li>Fine-tuning document conversations</li><li>Adding support for more formats</li><li>Enhancing conversation quality</li></ul><p>Stay tuned for more updates as we continue to enhance Vippsi‚Äôs capabilities!</p><p>~ Sven</p>",
      "date_published": "2025-01-02T00:00:00Z"
    }
    ,{
      "id": "http://localhost:8080/newsletter/welcome-to-my-new-digital-home/",
      "url": "http://localhost:8080/newsletter/welcome-to-my-new-digital-home/",
      "title": "Welcome to My New Digital Home",
      "content_html": "<p>Hey everyone! üëã</p><p>I‚Äôm excited to launch my new website where I‚Äôll be sharing my journey at the intersection of AI and trance music. This space will be my digital garden where I cultivate ideas, share experiences, and connect with like-minded people.</p><h2 id=\"what-to-expect\"><a href=\"#what-to-expect\" class=\"heading-anchor\">What to Expect</a></h2><p>Here‚Äôs what you‚Äôll find on this site:</p><ul class=\"list\"><li><strong>AI Leadership</strong>: Insights from my work leading the AI Platform Team at Vipps MobilePay, where we‚Äôre using AI to simplify complex workflows</li><li><strong>Trance Music</strong>: Weekly updates about my latest mixes, featuring both classic anthems and fresh progressive tunes</li><li><strong>Tech Insights</strong>: Thoughts about technology, leadership, and the future of AI</li><li><strong>Personal Updates</strong>: Stories and experiences from my life in Norway</li></ul><h2 id=\"why-this-site\"><a href=\"#why-this-site\" class=\"heading-anchor\">Why This Site?</a></h2><p>I believe in the power of sharing knowledge and experiences. Whether it‚Äôs discussing how we‚Äôre implementing AI solutions at Vipps MobilePay or sharing the story behind my latest trance mix, this site is where I‚Äôll keep track of my journey.</p><h2 id=\"stay-connected\"><a href=\"#stay-connected\" class=\"heading-anchor\">Stay Connected</a></h2><p>The best way to keep up with my content is to:</p><ul class=\"list\"><li>Follow me on <a href=\"https://www.linkedin.com/in/svenmalvik\" rel=\"noopener\">LinkedIn</a> for tech updates</li><li>Subscribe to my <a href=\"https://www.youtube.com/@svenmalvik\" rel=\"noopener\">YouTube channel</a> for weekly trance mixes</li><li>Check back here regularly for in-depth articles and insights</li></ul><p>Looking forward to sharing this journey with you!</p><p>~ Sven</p>",
      "date_published": "2025-01-01T00:00:00Z"
    }
    
  ]
}